{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e83dc-509b-486c-846b-dfc9581026ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2375777-43b9-4215-83f4-893a5bcb1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "#       aiplatform.HyperparameterTuningJob.from_local_script\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0657a237-f83b-4787-b73a-5a0ac0043f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# define the training script\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c25a073-0aa6-4b66-8802-1ea712fc362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile task.py\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "import os, json\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from google.cloud import storage\n",
    "import hypertune\n",
    "\n",
    "\n",
    "# parse args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--bucket_name', dest='bucket_name', default=\"\", type=str, help = 'The GCS bucket to store model artifacts -> w/o gs://')\n",
    "parser.add_argument('--max_depth', dest='max_depth', default=10, type=int, help = 'The maximum depth of the tree')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# load the iris dataset\n",
    "dataset = datasets.load_iris()\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data , dataset.target, test_size = 0.3, random_state = 7)\n",
    "\n",
    "# fit model, passing in the params being tuned\n",
    "model = XGBClassifier( max_depth = args.max_depth )\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# report metric for hyperparameter tuning\n",
    "hpt = hypertune.HyperTune()\n",
    "hpt.report_hyperparameter_tuning_metric(\n",
    "    hyperparameter_metric_tag='accuracy',\n",
    "    metric_value=accuracy\n",
    ")\n",
    "\n",
    "# save model to disk\n",
    "model_filename = \"model.bst\"\n",
    "model.save_model(model_filename)\n",
    "\n",
    "# save accuracy to disk\n",
    "metrics_filename = \"metrics.json\"\n",
    "with open(metrics_filename, \"w\") as f:\n",
    "    f.write( json.dumps( {\"accuracy\" : accuracy } ) )\n",
    "    \n",
    "# Upload to GCS\n",
    "storage_client = storage.Client()\n",
    "model_directory = os.environ[\"AIP_MODEL_DIR\"]\n",
    "\n",
    "# the model\n",
    "storage_path = os.path.join(model_directory, model_filename)\n",
    "blob = storage.blob.Blob.from_string(storage_path, client=storage_client)\n",
    "blob.upload_from_filename(model_filename)\n",
    "\n",
    "# the accruacy\n",
    "storage_path = os.path.join(model_directory, metrics_filename)\n",
    "blob = storage.blob.Blob.from_string(storage_path, client=storage_client)\n",
    "blob.upload_from_filename(metrics_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330ab95-6088-4791-9981-684b8b954252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# kick off the custom training job\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2daf61-6b49-4639-bd56-98c5935350cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3686d58a-f9bd-4d92-af11-a9a550730645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters\n",
    "P = ! gcloud config list --format 'value(core.project)'\n",
    "PROJECT_ID = P[0]\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = f\"bkt-{PROJECT_ID}-vpipelines\"\n",
    "BUCKET_PATH = f\"gs://{BUCKET_NAME}\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_PATH}/pipeline_root\"\n",
    "PIPELINE_DATA = f\"{BUCKET_PATH}/data\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87302d52-ec39-4388-a77f-a27a7166625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_PATH)\n",
    "\n",
    "# create the custom job\n",
    "job = aiplatform.CustomJob.from_local_script(\n",
    "    display_name = f\"vai_HyperparameterTuningJob_CustomJob_fromLocalScript_{TIMESTAMP}\"\n",
    "    , project = PROJECT_ID\n",
    "    , location = REGION\n",
    "    , script_path = \"task.py\"\n",
    "    , container_uri = \"us-docker.pkg.dev/vertex-ai/training/xgboost-cpu.1-1:latest\"\n",
    "    , requirements = [\"gcsfs\", \"pip==22.3.1\", \"cloudml-hypertune\"]\n",
    "    , replica_count = 1\n",
    "    , machine_type = \"n1-standard-4\"\n",
    "    , accelerator_count = 0\n",
    "    , args = [f\"--bucket_name={BUCKET_NAME}\"]\n",
    "    , environment_variables = { 'MY_KEY': 'MY_VALUE' }\n",
    "    , labels={'my_key': 'my_value'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636b248b-5e78-4214-8c91-f5e1848714a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_DISPLAY_NAME = f\"vai_HyperparameterTuningJob_{TIMESTAMP}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4645173b-bc1f-4ef3-bf4c-44d6bacec97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter job using the custom jobs\n",
    "hpt_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name = JOB_DISPLAY_NAME\n",
    "    , custom_job = job\n",
    "    , metric_spec = { \"accuracy\": \"maximize\", }\n",
    "    , parameter_spec = { \"max_depth\": hpt.IntegerParameterSpec(min = 1, max = 5, scale = \"linear\"), }\n",
    "    # The search algorithm to use: grid, random and None. \n",
    "    # If None is specified, the Vizier service (Bayesian) is used.\n",
    "    , search_algorithm = None\n",
    "    , max_trial_count = 2\n",
    "    , parallel_trial_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ef3be-ec4f-41e3-ac38-5c6cad5bcffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpt_job.run(service_account = f\"sa-vertex-pipelines@{PROJECT_ID}.iam.gserviceaccount.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b0fac-ccad-4f38-b78b-40eb55cdf030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best trial\n",
    "print( f\"HPT JOB NAME: {hpt_job.display_name}\")\n",
    "print(\"\")\n",
    "best = (None, None, None, 0.0)\n",
    "for trial in hpt_job.trials:\n",
    "    # Keep track of the best outcome\n",
    "    if float(trial.final_measurement.metrics[0].value) > best[3]:\n",
    "        try:\n",
    "            best = (\n",
    "                trial.id,\n",
    "                float(trial.parameters[0].value),\n",
    "                float(trial.parameters[1].value),\n",
    "                float(trial.final_measurement.metrics[0].value),\n",
    "            )\n",
    "        except:\n",
    "            best = (\n",
    "                trial.id,\n",
    "                float(trial.parameters[0].value),\n",
    "                None,\n",
    "                float(trial.final_measurement.metrics[0].value),\n",
    "            )\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24aea8b-b55c-4a75-94a7-3ef99b573fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model\n",
    "model_location = hpt_job.to_dict()['trialJobSpec']['baseOutputDirectory']['outputUriPrefix']\n",
    "BEST_MODEL_DIR = f\"{model_location}/{best[0]}/model\"\n",
    "! gsutil ls {BEST_MODEL_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9601572-6f1b-4315-a70f-6655a4db4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see info about the job\n",
    "hpt_job.to_dict()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
