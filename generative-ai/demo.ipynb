{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78679831-dcc3-460a-80bd-8737fce944e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b40556-9bf4-4f2f-9fce-7ce2d052a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "from langchain.llms import VertexAI\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35dcfbcf-2d8a-40ed-b0c7-c4b1cf9dd640",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = VertexAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc1064-a50a-48f7-b3b8-5241affd2d4c",
   "metadata": {},
   "source": [
    "# Summarization Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ea68a6e-76a0-4826-bea5-082ec893ea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words in the document = 13622\n"
     ]
    }
   ],
   "source": [
    "loader = WebBaseLoader(\n",
    "    \"https://cloud.google.com/blog/products/ai-machine-learning/how-to-use-grounding-for-your-llms-with-text-embeddings\"\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"# of words in the document = {len(documents[0].page_content)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63204b48-d735-44c3-8f2b-8b708929cd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This blog post discusses how to use grounding for your LLMs with text embeddings. Grounding is a technique that helps LLMs to better understand the meaning of words and phrases by associating them with real-world entities. This can be done by using text embeddings, which are vector representations of words that capture their semantic meaning.\n",
      "The post provides a step-by-step guide on how to use grounding for your LLMs with text embeddings. The steps are as follows:\n",
      "1. Train a text embedding model.\n",
      "2. Create a dataset of grounded text embeddings.\n",
      "3. Fine-tune your LLM on the dataset of grounded\n"
     ]
    }
   ],
   "source": [
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# There is a lot of complexity hidden in this one line. I encourage you to check out the video above for more detail\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced18d19-87fe-4c7b-bf57-a8f9da6a5037",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=False)\n",
    "print(chain.run(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02445a05-c399-40cd-a26e-8cb61f6631d1",
   "metadata": {},
   "source": [
    "# Question Answer Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ac2b8a-99a0-47c4-b8ca-2b1770c70c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest PDF files\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load GOOG's 10K annual report (92 pages).\n",
    "url = \"https://abc.xyz/investor/static/pdf/20230203_alphabet_10K.pdf\"\n",
    "loader = PyPDFLoader(url)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2074aaee-0140-4047-8296-34f3e4200df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents = 263\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"# of documents = {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09f23279-59d8-4efd-827e-d33c1b389b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI SDK version: 1.31.0\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "# Langchain\n",
    "#import langchain\n",
    "from pydantic import BaseModel\n",
    "\n",
    "#print(f\"LangChain version: {langchain.__version__}\")\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfda8168-7f12-443b-a4f4-a56b062ac130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for Embeddings API with rate limiting\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
    "    requests_per_minute: int\n",
    "    num_instances_per_batch: int\n",
    "\n",
    "    # Overriding embed_documents method\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        docs = list(texts)\n",
    "\n",
    "        while docs:\n",
    "            # Working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            head, docs = (\n",
    "                docs[: self.num_instances_per_batch],\n",
    "                docs[self.num_instances_per_batch :],\n",
    "            )\n",
    "            chunk = self.client.get_embeddings(head)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1b00594-1f67-4062-bb5e-4799c354b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "EMBEDDING_QPM = 100\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "embeddings = CustomVertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e474fb48-fd2f-40f8-bb99-cdbd9dd99d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomVertexAIEmbeddings(client=<vertexai.language_models._language_models._PreviewTextEmbeddingModel object at 0x7f7acd676200>, model_name='textembedding-gecko', temperature=0.0, max_output_tokens=128, top_p=0.95, top_k=40, stop=None, project=None, location='us-central1', credentials=None, request_parallelism=5, max_retries=6, requests_per_minute=100, num_instances_per_batch=5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select embedding engine - we use Vertex PaLM Embeddings API\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1220765e-8c59-42fd-834c-e094f609e5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting\n",
      "...................................................."
     ]
    }
   ],
   "source": [
    "# Store docs in local vectorstore as index\n",
    "# it may take a while since API is rate limited\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb57611f-b3ee-4d2e-899f-4adbe0b2f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expose index to the retriever\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd9e5023-970e-42f1-bce3-4a30f43ae70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain to answer questions\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Uses LLM to synthesize results from the search index.\n",
    "# We use Vertex PaLM Text API for LLM\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acee4a28-f0d4-44f2-8ca2-04f39e980f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Alphabet's net income in 2022 was $59,972 million, a decrease of 21.1% from $76,033 million in 2021.\n"
     ]
    }
   ],
   "source": [
    "query = \"What was Alphabet's net income in 2022?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c69cfbb9-05b9-4c27-8608-6c73e51cf2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': \"What was Alphabet's net income in 2022?\", 'result': \" Alphabet's net income in 2022 was $59,972 million, a decrease of 21.1% from $76,033 million in 2021.\", 'source_documents': [Document(page_content='Alphabet Inc.\\nCONSOLIDATED STATEMENTS OF INCOME\\n(in millions, except per share amounts)\\n Year Ended December 31,\\n 2020 2021 2022\\nRevenues $ 182,527 $ 257,637 $ 282,836 \\nCosts and expenses:\\nCost of revenues  84,732  110,939  126,203 \\nResearch and development  27,573  31,562  39,500 \\nSales and marketing  17,946  22,912  26,567 \\nGeneral and administrative  11,052  13,510  15,724 \\nTotal costs and expenses  141,303  178,923  207,994 \\nIncome from operations  41,224  78,714  74,842 \\nOther income (expense), net  6,858  12,020  (3,514) \\nIncome before income taxes  48,082  90,734  71,328 \\nProvision for income taxes  7,813  14,701  11,356 \\nNet income $ 40,269 $ 76,033 $ 59,972 \\nBasic net income per share of Class A, Class B, and Class C stock $ 2.96 $ 5.69 $ 4.59 \\nDiluted net income per share of Class A, Class B, and Class C stock $ 2.93 $ 5.61 $ 4.56 \\nSee accompanying notes.Table of Contents Alphabet Inc.\\n48', metadata={'source': '/var/tmp/tmpsdxzo44j/tmp.pdf', 'page': 48}), Document(page_content='restricted stock units and other  0  (9,754)  0  (1)  (9,755) \\nRepurchases of stock  (530)  (3,404)  0  (55,892)  (59,296) \\nSale of interest in consolidated entities  0  35  0  0  35 \\nNet income  0  0  0  59,972  59,972 \\nOther comprehensive income (loss)  0  0  (5,980)  0  (5,980) \\nBalance as of December 31, 2022  12,849 $ 68,184 $ (7,603) $ 195,563 $ 256,144 \\nSee accompanying notes.Table of Contents Alphabet Inc.\\n50', metadata={'source': '/var/tmp/tmpsdxzo44j/tmp.pdf', 'page': 50})]}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84581aed-9962-4992-ae3f-f1f038575ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
