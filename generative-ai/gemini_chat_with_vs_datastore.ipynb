{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57caaf84-f8dd-4ae8-9e1a-cff966ade1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4904da27-9675-4007-b0fc-e74ae34f02bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "#\n",
    "# project prep\n",
    "#\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d181a6c-2901-4513-a234-81af2e87baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable any APIs\n",
    "#! gcloud services enable aiplatform.googleapis.com\n",
    "#! gcloud services enable discoveryengine.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f59cb-40f9-44ad-b982-400577f6ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs - google \n",
    "#! pip install -U -q --user google-cloud-aiplatform\n",
    "#! pip install -U -q --user google-cloud-discoveryengine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e0460-1af3-4f1b-8601-af2c9205cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "#\n",
    "# setup\n",
    "#\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c545af37-1698-466a-918a-f49b23e5a593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for Gemini chat model\n",
    "import vertexai\n",
    "\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f88980ea-c517-4140-b5a6-e8ee8a66669a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for RAG using vertex search\n",
    "import google.cloud.discoveryengine_v1 as discoveryengine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e686f1b7-e3a2-4dcb-ac24-2739aa103a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set params\n",
    "P = ! gcloud config list --format 'value(core.project)'\n",
    "PROJECT_ID = P[0]\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "369fbd6f-1b35-4acb-a786-b67c3914d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertex Agent Builder Data Store - layout parsed for chunks\n",
    "VS_LOCATION = 'global'\n",
    "\n",
    "# YOUR DATASTORE ID GOES HERE\n",
    "VS_DATASTORE_ID = \"datastore-1723784659341\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c37bc7f-3283-46f2-a99e-87a49ef54922",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize clients\n",
    "# vertex\n",
    "vertexai.init(project = PROJECT_ID, location = REGION)\n",
    "\n",
    "# discoveryengine\n",
    "API_ENDPOINT = dict(api_endpoint = (f'{VS_LOCATION}-' if VS_LOCATION != 'global' else '') + 'discoveryengine.googleapis.com')\n",
    "search_client = discoveryengine.SearchServiceClient(client_options = API_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be58ebf5-d40b-4f36-9ce1-5f4bd595a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# vertex models config\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d3245bb-335d-499b-bc35-c9608b2361cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model generation settings\n",
    "generation_config = {\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4247d9dc-19c2-42ac-967e-12a344d34c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "#\n",
    "# helper functions\n",
    "#\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d895c54-6c67-4f51-a011-3dced1ddc678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vertex_search_chunks(PROJECT_ID\n",
    "                             , VS_LOCATION\n",
    "                             , VS_DATASTORE_ID\n",
    "                             , prompt):\n",
    "    \n",
    "    # SEARCH\n",
    "    search_results = search_client.search(\n",
    "        request = discoveryengine.SearchRequest(\n",
    "            serving_config = f\"projects/{PROJECT_ID}/locations/{VS_LOCATION}/collections/default_collection/dataStores/{VS_DATASTORE_ID}/servingConfigs/default_config\",\n",
    "            query = prompt,\n",
    "            #filter = 'myfilters: ANY(\"ariya\")', # could use this filter to isolate make, model, year if desired\n",
    "            page_size = 10, \n",
    "            content_search_spec = discoveryengine.SearchRequest.ContentSearchSpec(\n",
    "                snippet_spec = discoveryengine.SearchRequest.ContentSearchSpec.SnippetSpec(\n",
    "                    return_snippet = False\n",
    "                ),\n",
    "                \n",
    "                # this line is needed to return the chucks from the vertex datastore\n",
    "                search_result_mode = discoveryengine.SearchRequest.ContentSearchSpec.SearchResultMode.CHUNKS,\n",
    "\n",
    "                # generative summary specs:\n",
    "                # docs: https://cloud.google.com/python/docs/reference/discoveryengine/latest/google.cloud.discoveryengine_v1.types.SearchRequest.ContentSearchSpec.SummarySpec\n",
    "                summary_spec = discoveryengine.SearchRequest.ContentSearchSpec.SummarySpec(\n",
    "                    summary_result_count = 5, # number of documents (or chunks in chunk mode) to use for generative summary\n",
    "                    include_citations = True,\n",
    "                    ignore_adversarial_query = True,\n",
    "                    ignore_non_summary_seeking_query = False,\n",
    "                    model_spec = discoveryengine.SearchRequest.ContentSearchSpec.SummarySpec.ModelSpec(\n",
    "                        version = \"stable\"\n",
    "                    ),\n",
    "                ),\n",
    "                \n",
    "                # be sure to know how these params affect what is returned -> https://cloud.google.com/python/docs/reference/discoveryengine/latest/google.cloud.discoveryengine_v1.types.SearchRequest.ContentSearchSpec.ExtractiveContentSpec\n",
    "                extractive_content_spec = discoveryengine.SearchRequest.ContentSearchSpec.ExtractiveContentSpec(\n",
    "                    #max_extractive_answer_count = 1, # this could/should be zero bc you do not want vertex search generating the answer\n",
    "                    max_extractive_segment_count = 5,\n",
    "                    return_extractive_segment_score = True,\n",
    "                    num_previous_segments = 0,\n",
    "                    num_next_segments = 0\n",
    "                ),\n",
    "            ),\n",
    "            query_expansion_spec = discoveryengine.SearchRequest.QueryExpansionSpec(\n",
    "                condition = discoveryengine.SearchRequest.QueryExpansionSpec.Condition.AUTO\n",
    "            ),\n",
    "            spell_correction_spec = discoveryengine.SearchRequest.SpellCorrectionSpec(\n",
    "                mode = discoveryengine.SearchRequest.SpellCorrectionSpec.Mode.AUTO\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # PREP SEGMENTS - customize this however you want\n",
    "    retrieved_context = \"\"\n",
    "    for result_idx, result in enumerate(search_results.results):\n",
    "        retrieved_context += f\"<SEGMENT_{result_idx+1}>{result.chunk.content}</SEGMENT_{result_idx+1}>\"\n",
    "    \n",
    "    return retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2d3f982-7c6c-425c-af9f-0a30ce19220d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Demonstrate returned chunks from vertex search\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afeee988-531e-4b01-9353-4a1edcb9f2df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SEGMENT_1># Google models\n",
      "\n",
      "## Gemini models\n",
      "\n",
      "The following table summarizes the models available in the\n",
      "[Gemini API](/vertex-ai/generative-ai/docs/multimodal/overview). For more information about API details, see the\n",
      "[Gemini API reference](/vertex-ai/generative-ai/docs/model-reference/gemini). To explore a model in the Google Cloud console, select its model card in the Model Garden.\n",
      "\n",
      "_START_OF_TABLE_\n",
      "TABLE_IN_MARKDOWN:\n",
      "| Model | Inputs | Outputs | Use case | Try the model |\n",
      "|-|-|-|-|-|\n",
      "| Gemini 1.5 Flash | Text, code, images, audio, video, video with audio, PDF | Text | Provides speed and efficiency for high-volume, quality, cost-effective apps. | [Try the Gemini 1.5 Flash model](https://console.cloud.google.com/vertex-ai/generative/multimodal/create/text?model=gemini-1.5-flash-001) |\n",
      "| Gemini 1.5 Pro | Text, code, images, audio, video, video with audio, PDF | Text | Supports text or chat prompts for a text or code response. Supports long-context understanding up to the maximum input token limit. | [Try the Gemini 1.5 Pro model](https://console.cloud.google.com/vertex-ai/generative/multimodal/create/text?model=gemini-1.5-pro-001) |\n",
      "| Gemini 1.0 Pro | Text | Text | The best performing model for a wide range of text-only tasks. | [Go to the Gemini 1.0 Pro model card](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-pro) |\n",
      "| Gemini 1.0 Pro Vision | Text, images, audio, video, video with audio, PDF | Text | The best performing image and video understanding model to handle a broad range of applications. | [Try the Gemini 1.0 Pro Vision model](https://console.cloud.google.com/vertex-ai/generative/multimodal/create/text?model=gemini-1.0-pro-vision-001) |\n",
      "| Gemini 1.0 Ultra | Text | Text | The most capable text model, optimized for complex tasks, including instruction, code, and reasoning. | [Go to the Gemini 1.0 Ultra model card](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-ultra) |\n",
      "| Gemini 1.0 Ultra Vis\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Find information about gemini chat models\"\n",
    "retrieved_chunks_from_vertex_search = get_vertex_search_chunks(PROJECT_ID, VS_LOCATION, VS_DATASTORE_ID, prompt)\n",
    "print(retrieved_chunks_from_vertex_search[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50243ea2-f487-4bdf-8389-a91d6dd095ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "#\n",
    "# Chat model\n",
    "#\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8817986-adcd-4008-8821-213ebc00da18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the LLM\n",
    "#model_name = \"gemini-1.5-pro-001\"\n",
    "model_name = \"gemini-1.5-flash-001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c497c071-cc0b-45c7-805e-c56a2653cdce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# system instructions - persona\n",
    "system_instruction = \"You are a sales associate for Google Cloud focusing on machine learning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1933fdf8-93bf-497e-bbb2-9ba9fe163d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start chat\n",
    "chat_model_1 = GenerativeModel(model_name, system_instruction = [system_instruction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e8472e9-c2e3-4b8d-b9a3-a6e40a30d737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "#\n",
    "# Chat 1 - Initial Response\n",
    "#\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff6f5af3-cfd7-4de2-bf7c-cec725af2542",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_token_count: 7427\n",
      "candidates_token_count: 42\n",
      "total_token_count: 7469\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "The text-bison model (version 002) will be available until **October 9, 2024**. \n",
      "\n",
      "Is there anything else I can help you with today? \n",
      "\n",
      "CPU times: user 22.8 ms, sys: 2.11 ms, total: 24.9 ms\n",
      "Wall time: 2.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#######################################\n",
    "# putting it together\n",
    "#######################################\n",
    "\n",
    "# retrieve customer question from your interface\n",
    "customer_question = \"until what date is the text-bison model available?\"\n",
    "\n",
    "# instructions\n",
    "instruction_lines =  [  \"Determine and execute the steps that a customer engineer would utilize to answer the CUSTOMER_QUESTION with perfection.\"\n",
    "                      , \"Use only the SEGMENTS below to answer the CUSTOMER_QUESTION.\"\n",
    "                      , \"If the SEGMENTS do not contain enough information to answer the CUSTOMER_QUESTION, politely decline to answer and ask if there is any other way you can help.\"\n",
    "                     ]\n",
    "\n",
    "instructions = \" \".join(instruction_lines)\n",
    "\n",
    "# build the prompt\n",
    "chat_1_prompt =  f\"<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\"\n",
    "chat_1_prompt += f\"\\n\\n\"\n",
    "chat_1_prompt +=  f\"<CUSTOMER_QUESTION>{customer_question}</CUSTOMER_QUESTION>\"\n",
    "chat_1_prompt += f\"\\n\\n\"\n",
    "chat_1_prompt += f\"<SEGMENTS>{ get_vertex_search_chunks(PROJECT_ID, VS_LOCATION, VS_DATASTORE_ID, customer_question) }</SEGMENTS>\"\n",
    "\n",
    "# start the chat\n",
    "chat_1 = chat_model_1.start_chat()\n",
    "chat_response_1 = chat_1.send_message([chat_1_prompt]\n",
    "                      , generation_config=generation_config\n",
    "                      , safety_settings=safety_settings\n",
    "                     )\n",
    "\n",
    "chat_response_1_text = chat_response_1.candidates[0].content.parts[0].text\n",
    "\n",
    "print(chat_response_1.usage_metadata)\n",
    "print(\"\")\n",
    "print(\"-\"*30)\n",
    "print(\"\")\n",
    "print(chat_response_1_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7c4c777-cb2b-4b51-8667-52a35395466f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "#\n",
    "# Customer asks a second question...\n",
    "#\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df94a705-4d7e-44fc-8b91-d022c7225bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_token_count: 24192\n",
      "candidates_token_count: 490\n",
      "total_token_count: 24682\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Here's a breakdown of when to use LlamaIndex on Vertex AI for RAG versus LangChain on Vertex AI (Reasoning Engine) based on the provided information:\n",
      "\n",
      "**LlamaIndex on Vertex AI for RAG**\n",
      "\n",
      "* **Focus:** Primarily designed for **retrieval-augmented generation (RAG)**. It excels at building a knowledge base from your data (documents, files) and retrieving relevant information for use with LLMs.\n",
      "* **Strengths:**\n",
      "    * **Simple to use:**  Good for developers familiar with LlamaIndex.\n",
      "    * **Efficient retrieval:** Optimized for knowledge base construction and search.\n",
      "    * **Google-managed infrastructure:** Takes care of scaling, security, etc.\n",
      "* **When to use:**\n",
      "    * You have a large corpus of documents you need to make searchable and use for RAG.\n",
      "    * You need a straightforward, managed solution for building and querying a knowledge base.\n",
      "\n",
      "**LangChain on Vertex AI (Reasoning Engine)**\n",
      "\n",
      "* **Focus:** Building **more complex, agentic, and interactive AI applications** that combine LLMs with tools and external APIs.\n",
      "* **Strengths:**\n",
      "    * **Highly customizable:**  Allows for more intricate logic, tool integration, and agent-like behavior.\n",
      "    * **Full control over orchestration:** You define how your LLM interacts with tools and data.\n",
      "    * **Built on Vertex AI infrastructure:**  Leverages Google's platform for scaling, security, and deployment.\n",
      "* **When to use:**\n",
      "    * You need to build an AI application that can interact with external services or data sources.\n",
      "    * You want granular control over how your LLM interacts with the world.\n",
      "    * You need to build a complex agent with reasoning and decision-making capabilities.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "* **LlamaIndex on Vertex AI:** Choose for knowledge base creation and retrieval-focused RAG applications.\n",
      "* **LangChain on Vertex AI:** Choose for building complex, agentic AI applications that need to interact with external systems or data.\n",
      "\n",
      "**Important Note:** Both LlamaIndex and LangChain are under active development, so the best choice might change over time. It's always good to check the latest documentation and updates before making a decision. \n",
      "\n",
      "Let me know if you'd like more information about either of these frameworks or want help determining the best fit for your project! \n",
      "\n",
      "CPU times: user 16.7 ms, sys: 5.62 ms, total: 22.3 ms\n",
      "Wall time: 5.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# retrieve customer question from your interface\n",
    "customer_question = \"When would I use LlamaIndex on Vertex AI for RAG versus LangChain on Vertex AI aka Reasoning Engine?\"\n",
    "\n",
    "# instructions could change\n",
    "instruction_lines =  [  \"Determine and execute the steps that a customer engineer would utilize to answer the CUSTOMER_QUESTION with perfection.\"\n",
    "                      , \"Use only the SEGMENTS below to answer the CUSTOMER_QUESTION.\"\n",
    "                      , \"If the SEGMENTS do not contain enough information to answer the CUSTOMER_QUESTION, politely decline to answer and ask if there is any other way you can help.\"\n",
    "                     ]\n",
    "\n",
    "instructions = \" \".join(instruction_lines)\n",
    "\n",
    "# build the prompt\n",
    "chat_continuance =  f\"<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\"\n",
    "chat_continuance += f\"\\n\\n\"\n",
    "chat_continuance +=  f\"<CUSTOMER_QUESTION>{customer_question}</CUSTOMER_QUESTION>\"\n",
    "chat_continuance += f\"\\n\\n\"\n",
    "chat_continuance += f\"<SEGMENTS>{ get_vertex_search_chunks(PROJECT_ID, VS_LOCATION, VS_DATASTORE_ID, customer_question) }</SEGMENTS>\"\n",
    "\n",
    "\n",
    "chat_response_1 = chat_1.send_message([chat_continuance]\n",
    "                      , generation_config=generation_config\n",
    "                      , safety_settings=safety_settings\n",
    "                     )\n",
    "\n",
    "chat_continuance_response_text = chat_response_1.candidates[0].content.parts[0].text\n",
    "\n",
    "print(chat_response_1.usage_metadata)\n",
    "print(\"\")\n",
    "print(\"-\"*30)\n",
    "print(\"\")\n",
    "print(chat_continuance_response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d4efcd-5cfe-44d8-b854-a67d62c2357f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
