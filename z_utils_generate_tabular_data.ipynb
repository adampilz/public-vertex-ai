{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6517cd-3924-4bfc-aeeb-e1cc9db39f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# generate tabular classification datasets\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb8f920-ccae-47a8-95f8-1f1558f8d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fdcf962-4932-4552-8179-ef8c0e1e08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8f83a7-5952-43d7-89d1-84c90315d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters\n",
    "P = ! gcloud config list --format 'value(core.project)'\n",
    "PROJECT_ID = P[0]\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = f\"bkt-{PROJECT_ID}-central1-data\"\n",
    "BUCKET_PATH = f\"gs://{BUCKET_NAME}\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e4b0872-4ccb-42d0-b4cd-1ef6ae846e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# generate data\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49a0cd98-ca35-445e-aa24-27f9671a9991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn make_classification\n",
    "n_rows = 1000\n",
    "n_cols = 8\n",
    "n_informative = 6\n",
    "n_redundant = 2\n",
    "n_repeated = 0\n",
    "seed = 3816\n",
    "\n",
    "# additional random cols\n",
    "n_random_cols = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b307cc8c-a443-4663-aabc-ba69a8c3ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = make_classification(  n_samples = n_rows\n",
    "                         , n_features = n_cols\n",
    "                         , n_informative = n_informative\n",
    "                         , n_redundant = n_redundant\n",
    "                         , n_repeated = n_repeated\n",
    "                         , n_classes = 2\n",
    "                         , n_clusters_per_class = 2\n",
    "                         , weights = None\n",
    "                         , flip_y = 0.01\n",
    "                         , shift = 0.0\n",
    "                         , scale = 1.0\n",
    "                         , random_state = seed\n",
    "                         , shuffle = True\n",
    "                        )\n",
    "\n",
    "inputs_informative = ds[0]\n",
    "labels = ds[1].reshape((n_rows, 1))\n",
    "\n",
    "# random \n",
    "inputs_random = np.random.rand(n_rows, n_random_cols)\n",
    "\n",
    "# combine\n",
    "inputs = np.append(inputs_informative, inputs_random, axis = 1)\n",
    "data = np.append(labels, inputs, axis = 1)\n",
    "\n",
    "# headers write\n",
    "informative_col_headers = \",\".join([f\"x_inf_{i}\" for i in range(1, n_informative + 1)])\n",
    "redundant_col_headers = \",\".join([f\"x_red_{i}\" for i in range(1, n_redundant + 1)])\n",
    "# non repeated right now, add that here if needed\n",
    "random_col_headers = \",\".join([f\"x_ran_{i}\" for i in range(1, n_random_cols + 1)])\n",
    "header = \"label,\" + informative_col_headers + \",\" + redundant_col_headers + \",\" + random_col_headers\n",
    "\n",
    "data_filename = \"data.csv\"\n",
    "np.savetxt(data_filename, data, delimiter=',', header = header, comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90e8891d-4ecc-4c75-b7df-105b69101a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# upload to bigquery and GCS\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bcdcd08-8794-4162-b58f-b2716759d174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 rows into ds_central1:/projects/ap-alto-ml-1000/datasets/ds_central1/tables/tab_class_10inps_1krows_oos_3816.\n"
     ]
    }
   ],
   "source": [
    "# specify parameters\n",
    "dataset_id = \"ds_central1\" # bq ds ID\n",
    "\n",
    "data_type = \"tab\" # tab = tabular\n",
    "problem_type = \"class\"\n",
    "input_total = \"10\"\n",
    "rows_total = \"1k\"\n",
    "split = \"oos\" # tra tes val oos\n",
    "random_seed = seed\n",
    "table_id = f\"{data_type}_{problem_type}_{input_total}inps_{rows_total}rows_{split}_{random_seed}\"\n",
    "\n",
    "# run the functions below\n",
    "bq_write(PROJECT_ID, REGION, dataset_id, table_id, data_filename)\n",
    "write_gcs(PROJECT_ID, BUCKET_NAME, data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b49415c-a0a7-4324-b301-450ba41df336",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "#      helper functions\n",
    "############################################################################################################\n",
    "def bq_write(PROJECT_ID, REGION, dataset_id, table_id, data_filename):\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    client = bigquery.Client(location = REGION, project = PROJECT_ID)\n",
    "\n",
    "    dataset = client.get_dataset(dataset_id)\n",
    "    table_ref = dataset.table(table_id)\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV\n",
    "        , create_disposition = \"CREATE_IF_NEEDED\"\n",
    "        , skip_leading_rows = 1\n",
    "        , write_disposition = \"WRITE_TRUNCATE\" # WRITE_TRUNCATE WRITE_APPEND\n",
    "        , autodetect = True\n",
    "    )\n",
    "\n",
    "    with open(data_filename, 'rb') as source_file:\n",
    "        job = client.load_table_from_file(\n",
    "            source_file\n",
    "            , table_ref\n",
    "            , location= REGION\n",
    "            ,  job_config=job_config\n",
    "        )\n",
    "\n",
    "    job.result()\n",
    "\n",
    "    print('Loaded {} rows into {}:{}.'.format(\n",
    "        job.output_rows, dataset_id, table_ref.path))\n",
    "    \n",
    "def write_gcs(PROJECT_ID, BUCKET_NAME, data_filename):\n",
    "    from google.cloud import storage\n",
    "    storage_client = storage.Client(project = PROJECT_ID)\n",
    "    bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "    storage_path = os.path.join(\"gs://\", bucket.name, table_id + \".csv\")\n",
    "    blob = storage.blob.Blob.from_string(storage_path, client=storage_client)\n",
    "    blob.upload_from_filename(data_filename)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
