{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b523328-e216-4c1e-be5c-0534e01f90b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e6e03c-88c3-4ed5-ae4e-72b7afc82845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "#   aiplatform.CustomContainerTrainingJob\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef477a-70cc-4937-9d56-342bbf9881aa",
   "metadata": {},
   "source": [
    "# Create the training file/s and the dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794af46-5d52-44ba-b9ab-f81a2a5fcec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory\n",
    "! mkdir -p devdir/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8547ba8b-05ad-4ff0-a605-e60980130aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------\n",
    "# task.py\n",
    "#-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04807108-926c-410b-b062-4bed88e5c5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile devdir/trainer/task.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from google.cloud import storage\n",
    "\n",
    "# parse args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--bucket_name', dest='bucket_name', default=\"\", type=str, help='The GCS bucket to store model artifacts -> w/o gs://')\n",
    "parser.add_argument('--train_ds', dest='train_ds', default=\"\", type=str, help='file name of the training data')\n",
    "args = parser.parse_args()\n",
    "\n",
    "## set params\n",
    "DATA_BUCKET_NAME = args.bucket_name\n",
    "DATA_BUCKET_ROOT = f\"/gcs/{DATA_BUCKET_NAME}\"\n",
    "\n",
    "DS_TRAIN_FILENAME = args.train_ds\n",
    "DS_TRAIN_PATH = f\"{DATA_BUCKET_ROOT}/{DS_TRAIN_FILENAME}\"\n",
    "\n",
    "MODEL_DIR = os.environ[\"AIP_MODEL_DIR\"]\n",
    "JOB_TYPE = os.environ[\"JOB_TYPE\"]\n",
    "\n",
    "# Upload the saved model file to GCS -> using the client library\n",
    "my_filename = f\"info_{JOB_TYPE}.txt\"\n",
    "my_filename = f\"{DATA_BUCKET_ROOT}/info_{JOB_TYPE}.txt\"\n",
    "with open(my_filename, 'w') as f:\n",
    "    f.write( DATA_BUCKET_NAME + \"\\n\")\n",
    "    f.write( DATA_BUCKET_ROOT + \"\\n\")\n",
    "    f.write( DS_TRAIN_FILENAME + \"\\n\")\n",
    "    f.write( DS_TRAIN_PATH + \"\\n\")\n",
    "    f.write( MODEL_DIR + \"\\n\")\n",
    "    f.write( JOB_TYPE + \"\\n\")\n",
    "    f.write( \"\\n\")\n",
    "    f.write('-'*30)\n",
    "    f.write( \"\\n\")\n",
    "    f.write( \"ENV_VARS\" + \"\\n\")\n",
    "    for k, v in sorted(os.environ.items(), key=lambda x:x[0]):\n",
    "        f.write(f\"{k}={v}\" + \"\\n\")    \n",
    "    f.write(\"\\n\")\n",
    "    f.write('-'*30)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"TOP_LEVEL_LISTDIR\")\n",
    "    f.write(\"\\n\")\n",
    "    for i in os.listdir():\n",
    "        f.write(i+\"\\n\")        \n",
    "        \n",
    "#storage_client = storage.Client()\n",
    "#storage_path = os.path.join(MODEL_DIR, my_filename)\n",
    "#blob = storage.blob.Blob.from_string(storage_path, client=storage_client)\n",
    "#blob.upload_from_filename(my_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dfde9c-77cd-4d73-82ec-d99d5081df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------\n",
    "# Dockerfile\n",
    "#-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aae14b-7141-4bc1-ac20-ab7b7b8e1fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile devdir/Dockerfile\n",
    "\n",
    "# Specifies base image and tag\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-8\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# Installs additional packages\n",
    "RUN pip install pip --upgrade\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e218a-f3bf-40e8-b16e-0a8e7de017bc",
   "metadata": {},
   "source": [
    "# Build the docker file and push to artifact registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e02aeef-7d52-4436-b1da-fe6026d469d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3f395-1b24-46f7-a960-b4ce8b921104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters\n",
    "P = ! gcloud config list --format 'value(core.project)'\n",
    "PROJECT_ID = P[0]\n",
    "P = ! gcloud projects list --filter=\"$(gcloud config get-value project)\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUMBER = P[0]\n",
    "REGION = \"us-central1\"\n",
    "SERVICE_ACCOUNT = f\"sa-vertex-pipelines@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "\n",
    "# exercise details\n",
    "USE_CASE = \"CustomContainerTrainingJob\"\n",
    "ML_FRAMEWORK = \"scikit\"\n",
    "MODEL_TYPE = \"binclass\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# repo details\n",
    "REPO_NAME = f\"{USE_CASE}\"\n",
    "REPO_FORMAT = \"docker\"\n",
    "REPO_DESCRIPTION = f\"'Description for {REPO_NAME}'\"\n",
    "IMAGE_URI = f\"{REGION}-{REPO_FORMAT}.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{USE_CASE}:latest\"\n",
    "\n",
    "\n",
    "# GCS inputs\n",
    "GCS_BUCKET_NAME = f\"bkt-{REGION}-{USE_CASE}\"\n",
    "GCS_BUCKET_PATH = f\"gs://{GCS_BUCKET_NAME}\"\n",
    "GCS_BUCKET_PATH_DATA = f\"{GCS_BUCKET_PATH}/data\"\n",
    "GCS_BUCKET_PATH_CONFIGS = f\"{GCS_BUCKET_PATH}/configs\"\n",
    "GCS_BUCKET_PATH_TMP = f\"{GCS_BUCKET_PATH}/tmp\"\n",
    "GCS_BUCKET_PATH_STAGING = f\"{GCS_BUCKET_PATH}/staging\"\n",
    "\n",
    "TRAIN_DS = \"tab_class_10inps_1krows_tes_3498.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59460b7e-8beb-41c8-9d4b-bdef1736feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the repo and configure\n",
    "! gcloud artifacts repositories create {REPO_NAME} --repository-format={REPO_FORMAT} --location={REGION} --description={REPO_DESCRIPTION}\n",
    "! gcloud auth configure-{REPO_FORMAT} {REGION}-{REPO_FORMAT}.pkg.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0511940c-2918-46e4-8004-79b3bec19fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and push\n",
    "! docker build ./devdir/ -t {IMAGE_URI}\n",
    "! docker push {IMAGE_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596b863-e032-4441-afe1-385d354e3223",
   "metadata": {},
   "source": [
    "# Kick off CustomContainerTrainingJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75159f6b-a907-4336-931d-24d11d7c1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf70193-f47d-4cfc-b85a-b24a940a6444",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name = f\"CustomContainerTrainingJob_{TIMESTAMP}\"\n",
    "    , project = PROJECT_ID\n",
    "    , location = REGION\n",
    "    , staging_bucket = GCS_BUCKET_PATH\n",
    "    , container_uri = IMAGE_URI\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e254ce-1613-4194-ac12-4b5f7bebc30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run(service_account = SERVICE_ACCOUNT\n",
    "        #, network = f\"projects/{PROJECT_NUMBER}/global/networks/{NETWORK}\"\n",
    "        , replica_count = 1\n",
    "        , machine_type = \"n1-standard-4\"\n",
    "        #, model_display_name = \"mymodelname\"\n",
    "        #, model_labels = { 'MY_KEY': 'MY_VALUE' }\n",
    "        , args = [f\"--bucket_name={GCS_BUCKET_NAME}\", f\"--train_ds={TRAIN_DS}\"]\n",
    "        , environment_variables = { 'JOB_TYPE': 'CustomContainerTrainingJob' }\n",
    "       )"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
